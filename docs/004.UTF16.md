# UTF-16, UTF-16BE and UTF-16LE Encodings

Conclusions:

- UTF-16, UTF-16BE and UTF-16LE encodings are all variable-length 16-bit (2-byte) Unicode character encodings.
- Output byte streams of UTF-16 encoding may have 3 valid formats: Big-Endian without BOM, Big-Endian with BOM, and Little-Endian with BOM.
- UTF-16BE encoding is identical to the Big-Endian without BOM format of UTF-16 encoding.
- UTF-16LE encoding is identical to the Little-Endian with BOM format of UTF-16 encoding without using BOM.
- "UTF-16, an encoding of ISO 10646" at tools.ietf.org/html/rfc2781 gives official specifications of UTF-16, UTF-16BE and UTF-16LE encodings.

## What Are Paired Surrogates?

The goal UTF-16 encoding is to:

- Map Unicode code points in the range of `U+0000`~`0xFFFF` with 2 bytes (16 bits).
- Map Unicode code points in the range of `U+10000`~`0x10FFFF` with 4 bytes (32 bits).

The mapping for the `U+0000`~`0xFFFF` range is straightforward.

But the mapping for the `U+10000`~`0x10FFFF` range is tricky, because we want the resulting 4-byte stream can be recognized as 1 character in the `U+10000`~`0x10FFFF` range instead of 2 characters in the `U+0000`~`0xFFFF` range. This is achieved by using **paired surrogates**.

**What Are Paired Surrogates?** Paired surrogates are pairs of 2 16-bit unsigned integers in the **surrogate area** between `0xD800` and `0xDFFF`. Since there are no Unicode characters assigned with code points in the **surrogate area**, Paired surrogates can be easily recognized as 1 character in the `U+10000`~`0x10FFFF` range.

The UTF-16 specification defines that the first surrogate must be in the **high surrogate area** between `0xD800` and `0xDBFF` and the second surrogate in the **low surrogate area** between `0xDC00` and `0xDFFF`.

Based on my understanding of the specification, here is the algorithm to convert **a Unicode code point** in the range of `U+10000`~`0x10FFFF` to a surrogate pair:

- Let `U` be the unsigned integer value of the give code point.
- Let `U' = U - 0x10000`. `U'` is less than or equal to `0xFFFFF` and now can be expressed as an unassigned 20-bit integer.
- Divide 20 bits of `U'` into 2 blocks with 10 bits in each block as `0byyyyyyyyyyxxxxxxxxxx`.
- Let `S1 = 0xD800 + 0byyyyyyyyyy`, or `S1 = 0b110110yyyyyyyyyy`. `S1` is the first surrogate of the surrogate pair.
- Let `S2 = 0xDC00 + 0bxxxxxxxxxx`, or `S2 = 0b110111xxxxxxxxxx`. `S2` is the second surrogate of the surrogate pair.

## UTF-16 Encoding

Once we learned how to convert Unicode code points in the `U+10000`～`0x10FFFF` range into **paired surrogates**, we are ready to learn how UTF-16 encoding works.

**UTF-16**: A character encoding that maps **code points of Unicode character** set to **a sequence of 2 bytes (16 bits)**. **UTF-16** stands for **U**nicode **T**ransformation **F**ormat - **16**-bit.

Here is my understanding of the UTF-16 specification. When UTF-16 encoding is used to encode (serialize) Unicode characters into a byte stream for communication or storage, there are **3 valid optional formats**:

- **Big-Endian without BOM Format** - If the character is in the `U+0000`...`0xFFFF` range, convert the code point as an unassigned 16-bit integer into 2 bytes with the most significant byte first. If the character is in the `U+10000`...`0x10FFFF` range, convert the character into a **surrogate pair**, then convert each surrogate into 2 bytes with the most significant byte first.
- **Big-Endian with BOM Format** - Prepend `0xFEFF` first. Then convert each character in the same way as the Big-Endian without BOM Format.
- **Little-Endian with BOM Format** - Prepend `0xFFFE` first. Then convert each character in the same way as the Big-Endian without BOM Format except that 16-bit integers are converted into 2 bytes with the least significant byte first.

For example, all 3 encoding streams list below are valid UTF-16 encoded streams for 3 Unicode characters, `U+004D`, `U+0061` and `U+10000`:

- Big-Endian Format - 0x004D0061D800DC00
- Big-Endian with BOM Format - 0xFEFF004D0061D800DC00
- Little-Endian with BOM Format - 0xFFFE4D00610000D800DC

When UTF-16 encoding is used to decode (deserialize) a byte stream into Unicode characters, the following logic should be used:

- Step 1 - Read the first 2 bytes.
- Step 2a - If the first 2 bytes is `0xFEFF`, treat them as **BOM** (Byte Order Mark), and convert the rest of the byte stream in blocks of 2 bytes. Each block is converted to a 16-bit integer assuming **the most significant byte first**. Then process the converted integer stream according to Step 3a and 3b.
- Step 2b - If the first 2 bytes is `0xFFFE`, treat them as **BOM** (Byte Order Mark), and convert the rest of the byte stream in blocks of 2 bytes. Each block is converted to a 16-bit integer assuming **the least significant byte first**. Then process the converted integer stream according to Step 3a and 3b.
- Step 2c - If the first 2 bytes is not `0xFEFF` or `0xFFFE`, convert the entire stream, including the first 2 bytes, in blocks of 2 bytes. Each block is converted to a 16-bit integer assuming **the most significant byte first**. Then process the converted integer stream according to Step 3a and 3b.
- Step 3a - If a converted integer is not in the surrogate area, i.e. `< 0xD800` or `> 0xDFFF`, it represent the code point of the decode character.
- Step 3b - If a converted integer is in the surrogate area, i.e. `>= 0xD800` and `<= 0xDFFF`, it represent the first surrogate of a surrogate pair. Take the next converted integer as the second surrogate and convert the surrogate pair to a Unicode character in the `U+10000`～`0x10FFFF` range.

## UTF-16BE Encoding

**UTF-16BE**: A character encoding that maps code points of Unicode character set to a sequence of 2 bytes (16 bits). **UTF-16BE** stands for **Unicode Transformation Format - 16-bit Big Endian**.

Here is my understanding of the UTF-16BE specification. When UTF-16BE **encoding** is used to encode (serialize) Unicode characters into a byte stream for communication or storage, the resulting byte stream is identical to the **Big-Endian without BOM Format** of the **UTF-16 encoding**.

For example, these 3 Unicode characters, `U+004D`, `U+0061` and `U+10000` will be converted into `0x004D0061D800DC00` when UTF-16BE is used.

When UTF-16BE encoding is used to **decode** (deserialize) a byte stream into Unicode characters, the entire stream will be divided into blocks of 2 bytes. Each block is converted to a 16-bit integer assuming the most significant byte first. Then process the converted integer stream as described below:

- If a converted integer is not in the surrogate area, i.e. `< 0xD800` or `> 0xDFFF`, it represent the code point of the decode character.
- If a converted integer is in the surrogate area, i.e. `>= 0xD800` and `<= 0xDFFF`, it represent the first surrogate of a surrogate pair. Take the next converted integer as the second surrogate and convert the surrogate pair to a Unicode character in the `U+10000`～`0x10FFFF` range.

Note that the use of BOM (Byte Order Mark) is not part of the UTF-16BE specification. So you should:

- Not prepend BOM sequence, `0xFEFF`, to the output byte stream when **encoding**.
- Not treat initial sequence of `0xFEFF` as BOM when **decoding**. If it exists, convert the initial `0xFEFF` sequence as a Unicode character, the **ZERO WIDTH NO-BREAK SPACE**, `U+FEFF`, character.

## UTF-16LE Encoding

**UTF-16LE**: A character encoding that maps code points of Unicode character set to a sequence of 2 bytes (16 bits). **UTF-16LE** stands for **Unicode Transformation Format - 16-bit Little Endian**.

Here is my understanding of the UTF-16LE specification. When UTF-16LE encoding is used to **encode** (serialize) Unicode characters into a byte stream for communication or storage, the resulting byte stream is identical to the **Little-Endian with BOM Format** of the **UTF-16 encoding** except that BOM is not prepended to the byte stream.

For example, these 3 Unicode characters, `U+004D`, `U+0061` and `U+10000` will be converted into `0x4D00610000D800DC` when UTF-16LE is used.

When UTF-16LE encoding is used to **decode** (deserialize) a byte stream into Unicode characters, the entire stream will be divided into blocks of 2 bytes. Each block is converted to a 16-bit integer assuming **the least significant byte first**. Then process the converted integer stream as described below:

- If a converted integer is not in the surrogate area, i.e. `< 0xD800` or `> 0xDFFF`, it represent the code point of the decode character.
- If a converted integer is in the surrogate area, i.e. `>= 0xD800` and `<= 0xDFFF`, it represent the first surrogate of a surrogate pair. Take the next converted integer as the second surrogate and convert the surrogate pair to a Unicode character in the `U+10000`～`0x10FFFF` range.

Note that the use of BOM (Byte Order Mark) is not part of the UTF-16LE specification. So you should:

- Not prepend BOM sequence, `0xFFFE`, to the output byte stream when encoding.
- Not treat initial sequence of `0xFFFE` as BOM when decoding. If it exists, convert the initial `0xFFFE` sequence as a Unicode character.
